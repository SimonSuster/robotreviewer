"""
Lightweight classifier class for linear models trained elsewhere

Loads 'npz' files, which contain the model coefficients and
intercepts in sparse (csr) format. This allows very large models
(often several gigabytes in memory uncompressed) to be loaded
reasonably quickly, and makes for feasible memory usage.
"""

# Authors:  Iain Marshall <mail@ijmarshall.com>
#           Joel Kuiper <me@joelkuiper.com>
#           Byron Wallace <byron@ccs.neu.edu>
import torch
from scipy.sparse import csr_matrix
import numpy as np
import scipy
import logging
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
log = logging.getLogger(__name__)


class MiniClassifier:
    """
    Lightweight classifier
    Does only binary prediction using externally trained data
    """
    def __init__(self, filename):
        '''
        Models are compressed numpy files
        http://docs.scipy.org/doc/numpy/reference/generated/numpy.savez_compressed.html
        with the following keys:
            coef, intercept

            coef: a (1 x n) sparse matrix in csr format (where n
                  is the number of features)
            intercept: single element np.array containing float64

        Coefficients are immediately converted to the dense
        representation to speed up prediction (the .A1 bit returns
        the data contents of the numpy matrix as a numpy array,
        making calculations much quicker)

        Note that coefficients are 'arbitrarily' flipped from 
        positive to negative and vice-versa. The matching process
        will happen to corresponding values generated by any of the
        hashing vectorizers, and hence make the issue go away.
        '''
        log.debug("Loading model {}...".format(filename))
        with np.load(filename, allow_pickle=True, encoding='latin1') as raw_data:
            self.coef = raw_data["coef"].item().todense().A1
            self.intercept = raw_data["intercept"].item()
    
        log.debug("Model {} loaded".format(filename))

    def decision_function(self, X):
        scores = X.dot(self.coef.T) + self.intercept
        return scores

    def predict(self, X):
        scores = self.decision_function(X)
        return (scores>0).astype(np.int)

    def predict_proba(self, X):
        '''
        Note! This really only makes sense if the objective 
        for estimating w included a log-loss! Otherwise need 
        to calibrate.
        '''

        scores = self.decision_function(X)
        return sigmoid(scores)

    def predict_proba_logits(self, X):
        '''
        Also returns the logits.
        '''

        scores = self.decision_function(X)
        return sigmoid(scores), scores


def sigmoid(z):
    s = 1.0 / (1.0 + np.exp(-1.0 * z))
    return s


def sigmoid_torch(z):
    s = 1.0 / (1.0 + torch.exp(-1.0 * z))
    return s
